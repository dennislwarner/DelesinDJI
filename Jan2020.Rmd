---
title: "Andy's Observations of Jan 7 on The Estimated Distributions in the Forecast Wizard"
author: "Dennis Warner"
date: "2/21/2020"
output:
  
  html_document: default
  pdf_document: default
  always_allow_html: true
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE);
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.width = 8, 
                      fig.height = 4.5,
                      fig.align = 'center',
                      out.width='95%')
library(PerformanceAnalytics)
library(TTR)
library(R6);
library(Quandl);
library(stringr);
library(quantmod);
library(ggplot2);
library(dplyr);
# Get AAPL and AMZN, and MSFT Stock Prices and SPY and DIA ETF prices
source("D:/Projects/DDJI/0_ToolsForCharts.R",echo=FALSE);
source("D:/Projects/DDJI/0_AllFunctions.R",echo=FALSE);
source("D:/Projects/DDJI/0_ToolsForForeCastValues.R",echo=FALSE);
v.tickers<-c("SPY","AAPL","DRRX");
numTickers=length(v.tickers);

```
# Part I ---- The R TestBed  
### Data  
   Here we will use Adjusted Closes from the SPY etf and 2 stocks from July 13, 2017 through Jan 06, 2020. Taken from Quandl.  
```{r fetchData, echo=FALSE, message=FALSE, warning=FALSE}
  
GETLOCAL<-TRUE;
  if(GETLOCAL){
      ssl.P<-load("lP.RDATA");
  }else{
      l.P<-list();
      it<-0;
      while(it<numTickers){
        it<-it+1;
        symb<-v.tickers[it];
        df.p.xts<-f.getAdjustedClose(symb);
        l.P[[symb]]<-df.p.xts;
        cat("#",it," ",v.tickers[it]," Loaded", 
           as.character(first(index(df.p.xts))),
           " to ",
           as.character(last(index(df.p.xts))), "\n");
      }
      save(l.P,file="lP.RDATA");
  }
  
```

```{r combiner,echo=FALSE,message=FALSE,warning=FALSE}
#   COmbine the prices into one xts array

df.P.xts<-l.P[[1]];
it<-1;
while(it<numTickers){
  it<-it+1;
  symb<-v.tickers[it];
  df.p.xts<-l.P[[symb]]
  df.P.xts<-merge.xts(df.P.xts,df.p.xts);
}

```

###  Computing Returns  
     First compute the daily log-relative rates of return and limit the data to the desired span  July 13, 2017 through Jan 06, 2020. Their summary values are:

```{r makeRor, echo=FALSE}
#Build a daily ROR .xts frame for all securities
df.P<-f.xts2df(df.P.xts);
df.dR.xts<-mapXts(df.P.xts,f.roc);#Get Daily ROR for each series
#   limit the data to that originally observed
#
start      <- as.Date.character("2017-07-13")
end        <- as.Date.character("2020-01-06");
span       <- paste(start,"/",end,sep="");
firstHalf  <-paste("2017-10-09","/","2018-11-19",sep="");
secondHalf <-paste("2018-11-20","/","2020-01-06",sep="");
df.R.xts<-df.dR.xts[span,]
summary(100.0*df.R.xts)


```

### Holding Period Returns  

     Using the daily Rates of Return we mimic Morningstar and Calculate an M-Day holding period Rate of return as the differences between cumulative value of log-relative rates of return for day t, and day t-M.  This allows daily values for every day beginning with the M+1 st day through the final day in the series.  i.e. we calculate "rolling" holding period returns. 

----------------------------------------------------------------------------------------------------------

```{r holding period, echo=FALSE}
l.R <- list();
it<-0;
while(it<numTickers){
  it<-it+1;
  df.xts            <- f.myxtsHPror(df.P.xts[span,it],1,"Daily");
  df.xts$Weekly     <- f.myxtsHPror(df.P.xts[span,it],5,"Weekly");
  df.xts$Monthly    <- f.myxtsHPror(df.P.xts[span,it],21,"Monthly");
  df.xts$Quarterly  <- f.myxtsHPror(df.P.xts[span,it],63,"Quarterly");
  df.xts$Yearly     <- f.myxtsHPror(df.P.xts[span,it],252,"Yearly");
  
  # Apply the range to each price series to limit it to the desired dates only
  chart.TimeSeries(100*df.xts,main=v.tickers[it],legend.loc = 'topleft',ylab='Period Rates of Return',lwd=1);
  l.R[[v.tickers[it]]]<-df.xts[span,];   #Store only the time periods desired
}
```

###  The next task is to determine the best way to compute their standard deviations.  There are at least three ways:
1.  Direct Calculation of Overlapping Periods.  (This is the method the current implementation of the payload)
2.  Calculate using only non-overlapping Holding Periods. e.g. in a 756 trading-day span there are only 12 distinct period.
3.  Project the SD using the inverse square root law,  i.e.   SD(HP63) = SQRT(63)*SD(dailyRor);

The follwing tables illustrate the differences for the 3 securities under consideration.
```{r sdtable, echo=FALSE}
  dt<-f.Multimysd(l.R);
  dt

```

###There is a problem with the Overlapping method in that it does not give equal importance to all observations.  The weighting function would look like a trapezoid,  To illustrate imagine using a 3 day holding period in a 10 day long reference period.  The first available Holding Period value would be for days (1,2,3) , the second for days (2,3,4), etc.  There are 8 overlapping periods.  In constructing this sequence the first observation is used only once, the second is used twice, the 3rd through the 8th are used 3 times, the 9th is used twice, and the tenth is used once; hence observations 3 through 8 have much greater weight than observations 1,2,9, or 10.  It's hard to picture how this weighting scheme would effect the final results as the window is applied to different time series.  For longer holding periods , the weighting imbalance is only heightened.   

###A different problem occurs when the Independent (i.e. non-overlapping) holding periods are used.  In many cases there may not be enough observations to use for a reliable Standard Deviation Estimate.   In the illustrated case the 756 day long reference period holds 151 weekly, 36 monthly,  12 quarterly , and 3 annual observations.  The latter two are far frome adequate for estimating a standard deviatio with any confidence.

###Finally, the inverseq square law approximation seems to be the best bet.


###for the remainder of this note, we concentrate on the 63-day (Quarterly) holding period.  Given the Holding Period Rates, let's use the different measures to illustrate the importance of the standard deviation in estimated probability density charts.


```{r moments, echo=FALSE}



l.dt<-f.CompareStats(l.R);
l.dt$dtQ;  

l.dt$dtA;


```
  
 


### Distributions

    Using these values we can now compute the pdf and cdf for each stock.
    
```{r Distributions,echo=FALSE}
# df.WBEstimated<-read.csv("W:/Chartwork/GSPC_FDVEstimated.csv");  #Template
# df.FDVEst<-df.WBEstimated;
# names(df.FDVEst);

#Fill the estimated FDV
# pnorm   F(x) = P(XX,=x);  The cumulative probabiltiy Density Function
# qnorm   if p=F(x)   x=Finvers(p)  the inverse of the cumulative normal density function
# dnorm is the p.d.f. of the normal
df.stats<-data.frame(l.dt$dtQ$x$data)
l.res<-f.MakeDistributions2(v.tickers,l.dt$dtA$x$data);
dfGG<-l.res$SPY;
M<-length(l.res);  # number of securities
isym<-0;
while(isym<M){
  isym<-isym+1;
  symb<-names(l.res)[isym];
  #cat(isym,symb,"\n");
  
plottitle<-paste("Normal Densities of (Annualized) Holding Perfiod Rates of Return for ",symb)
p<-qplot(X,Y, data=dfGG,color=Period, geom=c("line","point"))+
  xlab('Holding Period RoR') + 
  ylab('density')+ 
  labs(title = plottitle);  
print (p);

plottitle<-paste("Cumulative Normal Densities of Annualized Rates of Return for ",symb);
p<-qplot(X,YC, data=dfGG,color=Period, geom=c("line","point"))+
  xlab('(Annualized) Holding Period RoR') + 
  ylab('Probability')+ 
  labs(title = plottitle);
  print (p);

}

```
    


# Part II ---- WorkBench Results

#### Here we use the reults exported from the workbench for the same data, reference period, and stocks.
The first task is to import the results

```{r ImportWorkBench, echo=FALSE}
# For each stock there are 4 files FDV for estimated and empirical, and FV for both
  
  l.WB<-f.GetWorkBenchSeries(v.tickers);
  names(l.WB)[1]<-v.tickers[1]<-"SPY";

```
 ### Then Show the Estimated Distributions from the WorkBench
 
```{r WBDist, echo=FALSE}
    v.periods<-c("First","Second","Total");
    isymb<-0;
    while(isymb<numTickers){
      isymb<-isymb+1;
      symb<-v.tickers[isymb];
      dfFDV<-(l.WB[[symb]])$df.FDVEst;#extract the Fore Dist Values for a stock
      #Stack the period subsets in a longer data.frame also grouped by Period
      df1<-dplyr::select(dfFDV,X=XFirst,Y=YFirst,YC=YFirstC);
      df2<-dplyr::select(dfFDV,X=XSecond,Y=YSecond,YC=YSecondC);
      dfT<-dplyr::select(dfFDV,X=XTotal,Y=YTotal,YC=YTotalC);
      dfOne<-data.frame(Period="First",df1);
      dfTwo<-data.frame(Period="Second",df2);
      dfTotal<-data.frame(Period="Total",dfT);
      dfGG<-rbind(dfOne,dfTwo,dfTotal);
      # iperiod<-0;
      # while(iperiod<length(v.periods)){
      #   iperiod<-iperiod+1;
      #   period<-v.periods[iperiod];
      #   dfg<-dplyr::select(dfFDV,X=XFirst,Y=YFirst,YC=YFirstC)%>%filter(,Period==period)
      #   if(iperiod==1){
      #     dfGG<-data.frame("Period"=period,dfg);
      #   }else{
      #     dfg<-data.frame("Period"=period,dfg)
      #     dfGG<-rbind(dfGG,dfg)
      #   }
      # }
      plottitle<-paste("Normal Densities of (WorkBench Annualized) Holding Period Rates of Return for ",symb)
p<-qplot(X,Y, data=dfGG,color=Period, geom=c("line","point"))+
  xlab('Holding Period RoR') + 
  ylab('density')+ 
  labs(title = plottitle);  
print (p);
      
plottitle<-paste("Cumulative Normal Densities of Annualized Rates of Return for ",symb);
p<-qplot(X,YC, data=dfGG,color=Period, geom=c("line","point"))+
  xlab('(Annualized) Holding Period RoR') + 
  ylab('Probability')+ 
  labs(title = plottitle);
  print (p);
    }

```
 



<!-- ##  Finally we look at the projected prices, where we  allow the user to set the target price, 1 holding period out, and we compute and display the compound rate of return that is required (or the user may set the rate and we show the consequent price).  Around the projected price path we draw a one-standard deviation cone of uncertainty. -->



<!-- #Get values from Workench -->
<!-- df.mySPY<-read.csv("W:/ChartWork/RawGSPC.csv"); -->

<!-- #Get values from Workench -->
<!-- df.FDVEst<-read.csv("W:/ChartWork/ForeDistValuesEstimated.csv"); -->
<!-- df.FDVEmp<-read.csv("W:/ChartWork/ForeDistValuesempirical.csv"); -->
<!-- df.FVEst<-read.csv("W:/ChartWork/FValuesEstimated.csv"); -->
<!-- df.FVEmp<-read.csv("W:/ChartWork/FValuesEmpirical.csv"); -->
<!-- ##Transform the FV time series into xts objects -->
<!-- df.FVEst.xts<-as.xts(df.FVEst,order.by=index(df.SPY.xts)); -->
<!-- df.FVEmp.xts<-as.xts(df.FVEmp,order.by=index(df.SPY.xts)); -->
<!-- df.SPYAllROR.xts<-allReturns(df.SPY.xts,type='log') -->
<!-- ``` -->




<!-- plot.xts(df.FVEmp.xts$HRor) -->


<!-- df.Estimated<-read.csv("W:/Chartwork/GSPCEstimated2.csv"); -->

<!-- df.mySPY.xts<-as.xts(df.SPY,order.by = index(df.SPY.xts)) -->
<!-- names(df.mySPY.xts)<-c("myAdj_Close","mydRor"); -->
<!-- df.myHRor.xts<-f.myxtsHPror(df.mySPY.xts$myAdj_Close,60,"myHPRor") -->

<!-- df.JointP<-merge.xts(df.SPY.xts,df.mySPY.xts); -->
<!-- summary(df.JointP) -->
<!-- df.J.xts<-merge.xts(df.SPY.xts,df.mySPY.xts); -->
<!-- df.J.xts<-merge.xts(df.J.xts,dr.xts) -->
<!-- df.J.xts<-merge.xts(df.J.xts,hr.xts) -->
<!-- df.J.xts<-merge.xts(df.J.xts,df.myHRor.xts$myHPRor) -->
<!-- #head(df.J.xts) -->
<!-- df.J.xts<-df.J.xts[-(1:61),] -->
<!-- summary(df.SPY.xts) -->
<!-- plot.xts(df.SPY.xts) -->
<!-- ``` -->

<!-- ### Convert this to a time series of 60-day holding period yields, and look at the first half, second half, and total period characteristics.NB, the first 60 periods are lost, leaving 565 of the original 625 observations for analysis. -->

<!-- ```{r conversion, echo = FALSE, message = FALSE, warning = FALSE} -->

<!-- #summary(df.myHRor.xts) -->
<!-- #summary(df.myHRor.xts[firstHalf,]); -->
<!-- #summary(df.myHRor.xts[secondHalf,]); -->


<!-- myMu<-mean(as.numeric(df.J.xts$myHPRor)); -->
<!-- mysd<-sd(df.J.xts$myHPRor); -->
<!-- xMuT<-mean(as.numeric(df.J.xts$HPRor)) -->
<!-- xsdT<-sd(df.J.xts$HPRor) -->

<!-- xMu1<-mean(df.J.xts[firstHalf,"HPRor"]); -->
<!-- xsd1<-sd(df.J.xts[firstHalf,"HPRor"]); -->

<!-- xMu2<-mean(df.J.xts[secondHalf,"HPRor"]); -->
<!-- xsd2<-mean(df.J.xts[secondHalf,"HPRor"]); -->

<!-- dfStats<-data.frame(matrix(0,3,3)); -->
<!-- names(dfStats)<-c("Period","Mean","StDev"); -->
<!-- dfStats[1,]<-c("Total",round(xMuT,digits=4),round(xsdT,digits=4)); -->
<!-- dfStats[2,]<-c("FirstHalf",round(xMu1,digits=4),round(xsd1,digits=4)); -->
<!-- dfStats[3,]<-c("SecondHalf",round(xMu2,digits=4),round(xsd2,digits=4)); -->

<!-- dfStats; -->
<!-- plot.xts(df.myHRor.xts,main="SPY 60 Day Holding Period Rates of Return") -->
<!-- ``` -->

<!-- ### Now derive the distributions -->

<!-- ```{r} -->
<!-- names(df.Estimated); -->
<!-- df.Estimated$Xtotal -->
<!-- pseq1<-seq(.01,.49,by=.02); -->
<!-- pseq2<-seq(.51,.99,by=.02); -->
<!-- pseq<-c(pseq1,0.50,pseq2); -->

<!-- m.X<-matrix(0,nrow=length(pseq),ncol=6); -->
<!-- m.Y<-m.X; -->


<!-- m.X[,1]<-qnorm(pseq,mean=xMuT, sd=xsdT); -->
<!-- m.Y[,1]<-dnorm(m.X[,1],mean=xMuT,sd=xsdT); -->

<!-- m.X[,2]<-qnorm(pseq,mean=xMu1,sd=xsd1) -->
<!-- m.Y[,2]<-dnorm(m.X[,2],mean=xMu1,sd=xsd1); -->

<!-- m.X[,3]<-qnorm(pseq,mean=xMu2,sd=xsd2); -->
<!-- m.Y[,3]<-dnorm(m.X[,3],mean=xMu2,sd=xsd2); -->

<!-- m.X[,4]<-df.Estimated$Xtotal; -->
<!-- m.Y[,4]<-df.Estimated$Ytotal; -->

<!-- m.X[,5]<-df.Estimated$Xfirst; -->
<!-- m.Y[,5]<-df.Estimated$Yfirst; -->

<!-- m.X[,6] <-df.Estimated$Xsecond; -->
<!-- m.Y[,6]<-df.Estimated$Ysecond; -->


<!-- df.GG<-data.frame("Prob"=pseq,m.X,m.Y); -->
<!-- names(df.GG)<-c("Prob","XT","X1","X2","myXT","myX1","myX2","YT","Y1","Y2","myYT","myY1","myY2"); -->
<!-- df.GG<-data.frame(m.X); -->
<!-- apply(df.GG,2,mean); -->
<!-- apply(df.GG,2,sd); -->
<!-- ggplot() +  -->
<!--   geom_line(datadfGG[dfGG$Period==1,] , aes(x = X, y = Y), color = "red") + -->
<!--   geom_line(data = dfGG[dfGG$Period==2,], aes(x = X, y = Y), color = "blue") + -->
<!--   geom_line(data = dfGG[dfGG$Period==0,], aes(x = X, y = Y), color = "black") + -->
<!--   xlab('Holding Period RoR') + -->
<!--   ylab('density')+ -->
<!--   labs(title = "Normal Densities"); -->
<!-- ``` -->


<!-- m.X[,2 -->
<!-- ] -->

<!-- ```{r PerformanceAnalytics, echo = FALSE, message = FALSE, warning = FALSE} -->

<!-- table.TrailingPeriods(df.SPYAllRates.xts, periods=c(1,5,21,63,252)) -->
<!-- dr<-df.SPYAllRates.xts$daily -->
<!-- fivenum(df.SPYAllRates.xts) -->
<!-- dr[is.na(dr)]<-0.0; -->
<!-- sd(dr$daily) -->
<!-- data(edhec) -->
<!--     sd.annualized(edhec) -->
<!--     sd.annualized(edhec[,6,drop=FALSE]) -->
<!--     # now for three periods: -->
<!--     sd.multiperiod(edhec[,6,drop=FALSE],scale=12) -->
<!--     sd.multiperiod(dr,scale=252) -->
<!--     sd.multiperiod(dr,scale=63) -->
<!-- ``` -->

